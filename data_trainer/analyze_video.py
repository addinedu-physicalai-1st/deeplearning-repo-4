import os
import sys
import cv2
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import matplotlib.gridspec as gridspec
from matplotlib.widgets import Button, Slider
import mediapipe as mp
from collections import deque
from PyQt6.QtWidgets import QApplication, QFileDialog

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import config

# train.pyì—ì„œ ëª¨ë¸ ìƒì„± í•¨ìˆ˜ ë° ìƒìˆ˜ë¥¼ ì„í¬íŠ¸
try:
    from data_trainer.train import create_model, INPUT_SHAPE, SEQUENCE_LENGTH, LANDMARKS_COUNT
except ImportError:
    # ê²½ë¡œ ë¬¸ì œ ì‹œ í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ì„í¬íŠ¸ ì‹œë„
    sys.path.append(os.path.dirname(__file__))
    from train import create_model, INPUT_SHAPE, SEQUENCE_LENGTH, LANDMARKS_COUNT

def normalize_landmarks(data):
    """
    ëœë“œë§ˆí¬ ì •ê·œí™”: ì†ëª©(ëœë“œë§ˆí¬ 0) ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ ë³€í™˜
    data: (frames, 21, 3)
    """
    wrist = data[:, 0:1, :]
    normalized = data - wrist
    scale = np.max(np.abs(normalized), axis=(1, 2), keepdims=True) + 1e-6
    normalized = normalized / scale
    return normalized

def analyze_video(video_path, model_path):
    print(f"ğŸ¬ ë¶„ì„ ì‹œì‘: {video_path}")
    
    # ë¼ë²¨ ë¡œë“œ
    labels_path = model_path.replace(".h5", "_labels.txt")
    if os.path.exists(labels_path):
        with open(labels_path, "r", encoding="utf-8") as f:
            classes = [line.strip() for line in f if line.strip()]
    else:
        classes = ["Pinch_In", "Pinch_Out", "Swipe_Left", "Swipe_Right"]
        print(f"âš ï¸ ë¼ë²¨ íŒŒì¼ ì—†ìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©: {classes}")

    # ëª¨ë¸ ì¬êµ¬ì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
    print(f"ğŸ§  ëª¨ë¸ êµ¬ì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ: {model_path}")
    try:
        model = create_model(len(classes))
        dummy_input = np.zeros((1, *INPUT_SHAPE), dtype=np.float32)
        model(dummy_input)
        model.load_weights(model_path)
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # ë¯¸ë””ì–´íŒŒì´í”„ ì„¤ì • (Drawing Utils í¬í•¨)
    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    mp_drawing_styles = mp.solutions.drawing_styles
    
    hands = mp_hands.Hands(
        static_image_mode=False,
        max_num_hands=1,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"â„¹ï¸ Video info: {fps:.2f} FPS, {total_frames} Frames")

    buffer = deque(maxlen=SEQUENCE_LENGTH)
    results = {cls: [] for cls in classes}
    timestamps = []
    frames = [] 

    frame_idx = 0
    print("â³ ë°ì´í„° ì²˜ë¦¬ ì¤‘... (ëœë“œë§ˆí¬ ì˜¤ë²„ë ˆì´ ë° ë¶„ì„)")
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # BGR -> RGB & Flip
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame_rgb = cv2.flip(frame_rgb, 1)

        result = hands.process(frame_rgb)
        
        # ëœë“œë§ˆí¬ ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸° (ì›ë³¸ì— ì˜¤ë²„ë ˆì´)
        annotated_image = frame_rgb.copy()
        
        landmarks_data = None
        if result.multi_hand_landmarks:
            hand_landmarks = result.multi_hand_landmarks[0]
            landmarks_data = [[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]
            
            # ì† ë¼ˆëŒ€ ê·¸ë¦¬ê¸°
            mp_drawing.draw_landmarks(
                annotated_image,
                hand_landmarks,
                mp_hands.HAND_CONNECTIONS,
                mp_drawing_styles.get_default_hand_landmarks_style(),
                mp_drawing_styles.get_default_hand_connections_style())
        else:
            landmarks_data = [[0.0, 0.0, 0.0] for _ in range(LANDMARKS_COUNT)]

        # ì‹œê°í™”ìš© ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        frames.append(annotated_image)

        # ë²„í¼ ë° ì¶”ë¡ 
        buffer.append(landmarks_data)
        
        probs = [0.0] * len(classes)
        if len(buffer) == SEQUENCE_LENGTH:
            data = np.array(buffer, dtype=np.float32) 
            data_normalized = normalize_landmarks(data)
            data_flat = data_normalized.reshape(SEQUENCE_LENGTH, -1)
            data_input = np.expand_dims(data_flat, axis=0)
            
            prediction = model.predict(data_input, verbose=0)
            probs = prediction[0]

        for i, cls in enumerate(classes):
            results[cls].append(probs[i])
        timestamps.append(frame_idx / fps)

        frame_idx += 1
        pct = (frame_idx / total_frames) * 100 if total_frames > 0 else 0
        if frame_idx % 30 == 0:
            print(f"\r   Progress: {pct:.1f}% ({frame_idx}/{total_frames})", end="")

    print("\nâœ… ë¶„ì„ ì™„ë£Œ. ì‹œê°í™” ì¤€ë¹„ ì¤‘...")
    cap.release()
    hands.close()

    # --- ì‹œê°í™” (ì¢Œì¸¡ ë¹„ë””ì˜¤ / ìš°ì¸¡ ê·¸ë˜í”„ 4í–‰) ---
    fig = plt.figure(figsize=(16, 9))
    num_classes = len(classes)
    
    gs = gridspec.GridSpec(num_classes, 2, width_ratios=[1.5, 1])

    # 1. ë¹„ë””ì˜¤ í™”ë©´
    ax_video = fig.add_subplot(gs[:, 0]) 
    ax_video.set_title(f"Video (Mirrored): {os.path.basename(video_path)}")
    ax_video.axis('off')
    if frames:
        im_video = ax_video.imshow(frames[0])
    
    # 2. ê·¸ë˜í”„
    axs_graphs = []
    lines = []
    cursors = []
    colors = ['#FF5733', '#33FF57', '#3357FF', '#F333FF', '#FF33A8']
    
    max_time = timestamps[-1] if timestamps else 1
    
    for i, cls in enumerate(classes):
        ax = fig.add_subplot(gs[i, 1])
        ax.text(1.02, 0.5, cls, transform=ax.transAxes, va='center', ha='left', fontsize=12, fontweight='bold', color=colors[i % len(colors)])
        
        ax.set_ylim(-0.1, 1.1)
        ax.set_xlim(0, max_time)
        ax.grid(True, linestyle=':', alpha=0.6)
        
        if i < num_classes - 1:
            ax.set_xticklabels([])
        else:
            ax.set_xlabel('Time (s)')
            
        line, = ax.plot(timestamps, results[cls], color=colors[i % len(colors)], linewidth=2, alpha=0.8)
        cursor = ax.axvline(x=0, color='red', linestyle='-', linewidth=1.5, alpha=0.9)
        ax.axhline(y=0.7, color='gray', linestyle='--', linewidth=0.8, alpha=0.5)
        
        axs_graphs.append(ax)
        lines.append(line)
        cursors.append(cursor)

    plt.tight_layout()
    plt.subplots_adjust(bottom=0.15) 

    # --- í”Œë ˆì´ì–´ ë¡œì§ ---
    class Player:
        def __init__(self):
            self.frame_idx = 0
            self.is_paused = False
            self.is_exporting = False

        def update(self, i):
            if self.is_exporting: # ë‚´ë³´ë‚´ê¸° ì¤‘ì—” ì™¸ë¶€ í”„ë ˆì„ ë²ˆí˜¸ ë”°ë¦„
                idx = i
            elif slider.eventson and slider.val != self.frame_idx and self.is_paused:
                 self.frame_idx = int(slider.val)
                 idx = self.frame_idx
            elif not self.is_paused:
                self.frame_idx = (self.frame_idx + 1) % len(frames)
                slider.eventson = False
                slider.set_val(self.frame_idx)
                slider.eventson = True
                idx = int(self.frame_idx)
            else:
                idx = int(self.frame_idx)
            
            if idx >= len(frames): idx = len(frames) - 1
            
            # ê·¸ë¦¬ê¸° ì—…ë°ì´íŠ¸
            im_video.set_data(frames[idx])
            current_time = timestamps[idx] if idx < len(timestamps) else 0
            for cursor in cursors:
                 cursor.set_xdata([current_time, current_time])
                
            return [im_video] + cursors

        def toggle_play(self, event):
            self.is_paused = not self.is_paused
            btn_play.label.set_text('Play' if self.is_paused else 'Pause')

        def on_slider_change(self, val):
            self.frame_idx = int(val)
            self.is_paused = True 
            btn_play.label.set_text('Play')
            
            im_video.set_data(frames[self.frame_idx])
            current_time = timestamps[self.frame_idx]
            for cursor in cursors:
                cursor.set_xdata([current_time, current_time])
            fig.canvas.draw_idle()

        def save_video(self, event):
            self.is_paused = True
            btn_play.label.set_text('Play')
            btn_save.label.set_text('Saving...')
            plt.draw()
            
            save_path = video_path + "_analyzed.mp4"
            print(f"\nğŸ’¾ ë¹„ë””ì˜¤ ì €ì¥ ì‹œì‘: {save_path}")
            
            self.is_exporting = True
            try:
                # FFMpeg Writer ì„¤ì •
                Writer = animation.writers['ffmpeg']
                writer = Writer(fps=fps, metadata=dict(artist='Gesto'), bitrate=1800)
                
                # ì• ë‹ˆë©”ì´ì…˜ì„ ìƒˆë¡œ ìƒì„±í•˜ì—¬ ì €ì¥ (í˜„ì¬ UI ìƒíƒœì™€ ë¶„ë¦¬)
                save_anim = animation.FuncAnimation(fig, self.update, frames=len(frames), blit=False)
                save_anim.save(save_path, writer=writer)
                print("âœ… ì €ì¥ ì™„ë£Œ!")
                btn_save.label.set_text('Saved!')
            except Exception as e:
                print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
                print("FFmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš” (sudo apt install ffmpeg).")
                btn_save.label.set_text('Error')
            
            self.is_exporting = False


    player = Player()

    # ë©”ì¸ ì• ë‹ˆë©”ì´ì…˜
    anim = animation.FuncAnimation(
        fig, 
        player.update, 
        frames=None, 
        interval=1000/fps, 
        blit=False, 
        cache_frame_data=False
    )

    # --- UI ë²„íŠ¼ ---
    # 1. Play/Pause
    ax_play = plt.axes([0.35, 0.02, 0.1, 0.05])
    btn_play = Button(ax_play, 'Pause')
    btn_play.on_clicked(player.toggle_play)
    
    # 2. Save Video
    ax_save = plt.axes([0.55, 0.02, 0.1, 0.05])
    btn_save = Button(ax_save, 'Save MP4')
    btn_save.on_clicked(player.save_video)

    # 3. Slider
    ax_slider = plt.axes([0.15, 0.08, 0.7, 0.03])
    slider = Slider(
        ax=ax_slider, 
        label='Frame', 
        valmin=0, 
        valmax=len(frames)-1, 
        valinit=0, 
        valstep=1,
        color='lightblue'
    )
    slider.on_changed(player.on_slider_change)

    print("ğŸ¥ ì¬ìƒ ì‹œì‘!")
    plt.show()

def select_file():
    app = QApplication(sys.argv)
    file_path, _ = QFileDialog.getOpenFileName(
        None, 
        "Select Video File", 
        os.path.join(os.path.dirname(__file__), "data"), 
        "Video Files (*.mp4 *.avi *.mov *.webm *.mkv);;All Files (*)"
    )
    return file_path

if __name__ == "__main__":
    # ëª¨ë¸ ê²½ë¡œ ìë™ íƒìƒ‰
    default_model_path = os.path.join(os.path.dirname(__file__), "models", "lstm_legacy.h5")
    
    if len(sys.argv) > 1:
        target_video = sys.argv[1]
    else:
        target_video = select_file()

    if target_video and os.path.exists(target_video):
        if os.path.exists(default_model_path):
            analyze_video(target_video, default_model_path)
        else:
            print(f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {default_model_path}")
            print("ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
            print("python train.py")
    else:
        print("No video selected.")
