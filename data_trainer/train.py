import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ê²½ë¡œ ì„¤ì •
DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'data_collector', 'data'))
LEGACY_DATA_DIR = os.path.join(DATA_DIR, 'legacy')
MODELS_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), 'models'))

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
SEQUENCE_LENGTH = 45  # 1.5ì´ˆ * 30fps
LANDMARKS_COUNT = 21
COORDS_COUNT = 3
INPUT_SHAPE = (SEQUENCE_LENGTH, LANDMARKS_COUNT * COORDS_COUNT)
EPOCHS = 100  # Early stoppingìœ¼ë¡œ ì‹¤ì œë¡œëŠ” ë” ì ê²Œ í•™ìŠµë  ìˆ˜ ìˆìŒ
BATCH_SIZE = 16

def normalize_landmarks(data):
    """
    ëœë“œë§ˆí¬ ì •ê·œí™”: ì†ëª©(ëœë“œë§ˆí¬ 0)ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
    
    Args:
        data: (frames, 21, 3) shapeì˜ numpy array
    
    Returns:
        ì •ê·œí™”ëœ ë°ì´í„° (frames, 21, 3)
    """
    # ì†ëª© ì¢Œí‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ì¢Œí‘œ ë³€í™˜
    wrist = data[:, 0:1, :]  # (frames, 1, 3)
    normalized = data - wrist  # ì†ëª© ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ
    
    # ìŠ¤ì¼€ì¼ ì •ê·œí™” (ì† í¬ê¸° ì°¨ì´ ë³´ì •)
    scale = np.max(np.abs(normalized), axis=(1, 2), keepdims=True) + 1e-6
    normalized = normalized / scale
    
    return normalized

def augment_data(X, y, augmentation_factor=2):
    """
    ë°ì´í„° ì¦ê°•: ë…¸ì´ì¦ˆ ì¶”ê°€ë¡œ í•™ìŠµ ë°ì´í„° ì¦ê°€
    
    Args:
        X: ì…ë ¥ ë°ì´í„° (N, 45, 63)
        y: ë ˆì´ë¸” ë°ì´í„° (N,)
        augmentation_factor: ì¦ê°• ë°°ìˆ˜ (ê¸°ë³¸ 2ë°°)
    
    Returns:
        ì¦ê°•ëœ X, y
    """
    X_aug = []
    y_aug = []
    
    for i in range(len(X)):
        # ì›ë³¸ ë°ì´í„°
        X_aug.append(X[i])
        y_aug.append(y[i])
        
        # ì¦ê°• ë²„ì „ë“¤
        for _ in range(augmentation_factor - 1):
            # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
            noise = np.random.normal(0, 0.01, X[i].shape)
            X_aug.append(X[i] + noise)
            y_aug.append(y[i])
    
    return np.array(X_aug), np.array(y_aug)

def load_data(data_dir, apply_normalization=True):
    """
    íŠ¹ì • ë””ë ‰í† ë¦¬(legacy ë˜ëŠ” tasks)ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    êµ¬ì¡°: data_dir/<Mode>/<GestureName>/*.npy
    ModeëŠ” ë¬´ì‹œí•˜ê³  GestureNameì„ ë ˆì´ë¸”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        apply_normalization: ì •ê·œí™” ì ìš© ì—¬ë¶€
    
    Returns:
        X, y, label_map
    """
    X = []
    y = []
    labels = []
    label_map = {}
    
    if not os.path.exists(data_dir):
        print(f"ê²½ê³ : ë””ë ‰í† ë¦¬ {data_dir}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return np.array(X), np.array(y), label_map

    # ì•Œë ¤ì§„ ëª¨ë“œ 'Gesture'ì™€ 'Posture' íƒìƒ‰
    modes = ['Gesture', 'Posture']
    
    current_label_id = 0
    
    for mode in modes:
        mode_path = os.path.join(data_dir, mode)
        if not os.path.exists(mode_path):
            continue
            
        gestures = os.listdir(mode_path)
        for gesture in gestures:
            gesture_path = os.path.join(mode_path, gesture)
            if not os.path.isdir(gesture_path):
                continue
                
            if gesture not in label_map:
                label_map[gesture] = current_label_id
                labels.append(gesture)
                current_label_id += 1
            
            label_id = label_map[gesture]
            
            # ëª¨ë“  .npy íŒŒì¼ ë¡œë“œ
            for file in os.listdir(gesture_path):
                if file.endswith('.npy'):
                    file_path = os.path.join(gesture_path, file)
                    try:
                        data = np.load(file_path)
                        # ë°ì´í„° ëª¨ì–‘ì€ (Frames, 21, 3)
                        
                        # ì •ê·œí™” ì ìš©
                        if apply_normalization:
                            data = normalize_landmarks(data)
                        
                        # SEQUENCE_LENGTH í”„ë ˆì„ì„ ê°–ë„ë¡ ë³´ì¥
                        if data.shape[0] > SEQUENCE_LENGTH:
                            data = data[:SEQUENCE_LENGTH]
                        elif data.shape[0] < SEQUENCE_LENGTH:
                            # 0ìœ¼ë¡œ íŒ¨ë”©
                            padding = np.zeros((SEQUENCE_LENGTH - data.shape[0], 21, 3))
                            data = np.vstack((data, padding))
                        
                        # ëœë“œë§ˆí¬ í‰íƒ„í™”: (45, 21, 3) -> (45, 63)
                        data_flat = data.reshape(SEQUENCE_LENGTH, -1)
                        
                        X.append(data_flat)
                        y.append(label_id)
                    except Exception as e:
                        print(f"Error loading {file_path}: {e}")
                        
    return np.array(X), np.array(y), label_map

def create_model(num_classes):
    """
    ê°œì„ ëœ LSTM ëª¨ë¸ ìƒì„±
    - ê²½ëŸ‰í™”ëœ ì•„í‚¤í…ì²˜ (ì¶”ë¡  ì†ë„ ê°œì„ )
    - Dropout ì¶”ê°€ (ê³¼ì í•© ë°©ì§€)
    - L2 ì •ê·œí™” ì¶”ê°€ (ì¼ë°˜í™” ì„±ëŠ¥ ê°œì„ )
    """
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=INPUT_SHAPE),
        Dropout(0.3),
        LSTM(64, return_sequences=False),
        Dropout(0.3),
        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer='Adam',
        loss='categorical_crossentropy',
        metrics=['categorical_accuracy']
    )
    
    return model

def save_tflite_model(model, save_path):
    """
    TensorFlow Lite ëª¨ë¸ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (Robust Version)
    
    Args:
        model: Keras ëª¨ë¸
        save_path: .h5 íŒŒì¼ ê²½ë¡œ
    """
    import tempfile
    
    tflite_path = save_path.replace('.h5', '.tflite')
    print(f"ğŸ”„ TFLite ë³€í™˜ ì‹œì‘...")
    
    try:
        # ì•ˆì „í•œ ë³€í™˜ì„ ìœ„í•´ ì„ì‹œ ë””ë ‰í† ë¦¬ì— SavedModelë¡œ ë¨¼ì € ì €ì¥
        # ì´ëŠ” Keras ëª¨ë¸ì„ ì§ì ‘ ë³€í™˜í•  ë•Œ ë°œìƒí•˜ëŠ” ê·¸ë˜í”„ ë™ê²° ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
        with tempfile.TemporaryDirectory() as temp_dir:
            model.export(temp_dir) # Keras 3 export ì‚¬ìš©
            
            # SavedModelì—ì„œ ì»¨ë²„í„° ìƒì„±
            converter = tf.lite.TFLiteConverter.from_saved_model(temp_dir)
            
            # TF Ops ì§€ì› ì¶”ê°€ (LSTM ë“± ë³µì¡í•œ ë ˆì´ì–´ í˜¸í™˜ì„± ë° LLVM ì—ëŸ¬ ë°©ì§€)
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS, # TFLite ê¸°ë³¸ Ops
                tf.lite.OpsSet.SELECT_TF_OPS    # TF Ops (í•„ìš”ì‹œ ì‚¬ìš©)
            ]
            
            # ì–‘ìí™” ë° ë³€í™˜ ì‹œë„
            try:
                converter.optimizations = [tf.lite.Optimize.DEFAULT]
                tflite_model = converter.convert()
                quantized = True
            except Exception as quant_error:
                print(f"âš ï¸ ì–‘ìí™” ì‹¤íŒ¨, ê¸°ë³¸ ë³€í™˜ ì‹œë„: {quant_error}")
                # ì»¨ë²„í„° ì¬ì„¤ì • (SavedModel ë‹¤ì‹œ ë¡œë“œ ë¶ˆí•„ìš”, ì˜µì…˜ë§Œ ë³€ê²½ ë¶ˆê°€í•˜ë¯€ë¡œ ì¬ìƒì„± ê¶Œì¥)
                converter = tf.lite.TFLiteConverter.from_saved_model(temp_dir)
                converter.target_spec.supported_ops = [
                    tf.lite.OpsSet.TFLITE_BUILTINS,
                    tf.lite.OpsSet.SELECT_TF_OPS
                ]
                tflite_model = converter.convert()
                quantized = False

            # íŒŒì¼ ì €ì¥
            with open(tflite_path, 'wb') as f:
                f.write(tflite_model)
            
            # ê²°ê³¼ ì¶œë ¥
            h5_size = os.path.getsize(save_path) / 1024  # KB
            tflite_size = os.path.getsize(tflite_path) / 1024  # KB
            print(f"âœ… TFLite ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {tflite_path}")
            print(f"   H5 ëª¨ë¸ í¬ê¸°: {h5_size:.2f} KB")
            print(f"   TFLite ëª¨ë¸ í¬ê¸°: {tflite_size:.2f} KB (ì••ì¶•ë¥ : {(1 - tflite_size/h5_size)*100:.1f}%)")
            print(f"   ëª¨ë“œ: {'ì–‘ìí™” (Quantized)' if quantized else 'ì¼ë°˜ (Float32)'} + TF Ops")
            
    except Exception as e:
        print(f"âŒ TFLite ë³€í™˜ ì‹¤íŒ¨: {e}")
        print(f"   H5 ëª¨ë¸ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def evaluate_model(model, X_test, y_test, label_map):
    """
    ëª¨ë¸ í‰ê°€ ë° ìƒì„¸ ë©”íŠ¸ë¦­ ì¶œë ¥
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        y_test: í…ŒìŠ¤íŠ¸ ë ˆì´ë¸” (one-hot encoded)
        label_map: ë ˆì´ë¸” ë§¤í•‘ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        confusion matrix
    """
    # ì˜ˆì¸¡
    y_pred = model.predict(X_test, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test_classes = np.argmax(y_test, axis=1)
    
    # Classification Report
    print("\n" + "="*70)
    print("ğŸ“Š Classification Report")
    print("="*70)
    
    # label_mapì„ ì •ë ¬í•˜ì—¬ target_names ìƒì„±
    sorted_labels = sorted(label_map.items(), key=lambda x: x[1])
    target_names = [label for label, _ in sorted_labels]
    
    print(classification_report(y_test_classes, y_pred_classes, 
                                target_names=target_names,
                                digits=4))
    
    # Confusion Matrix
    cm = confusion_matrix(y_test_classes, y_pred_classes)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=target_names,
                yticklabels=target_names)
    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    
    cm_path = os.path.join(MODELS_DIR, 'confusion_matrix.png')
    plt.savefig(cm_path, dpi=150)
    print(f"\nâœ… Confusion Matrix ì €ì¥ë¨: {cm_path}")
    plt.show()
    
    return cm

def train_model(X, y, save_path, model_name, apply_augmentation=True):
    """
    ê°œì„ ëœ ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
    
    Args:
        X: ì…ë ¥ ë°ì´í„°
        y: ë ˆì´ë¸”
        save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        model_name: ëª¨ë¸ ì´ë¦„
        apply_augmentation: ë°ì´í„° ì¦ê°• ì ìš© ì—¬ë¶€
    
    Returns:
        history, model
    """
    if len(X) == 0:
        print(f"{model_name}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ì–´ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None, None

    # ë°ì´í„° ì¦ê°• (í•™ìŠµ ë°ì´í„°ê°€ ì ì„ ê²½ìš° ìœ ìš©)
    if apply_augmentation:
        print("ğŸ“ˆ ë°ì´í„° ì¦ê°• ì ìš© ì¤‘...")
        X, y = augment_data(X, y, augmentation_factor=2)
        print(f"   ì¦ê°• í›„ ë°ì´í„° ê°œìˆ˜: {len(X)}")

    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬)
    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weight_dict = dict(enumerate(class_weights))
    print(f"âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {class_weight_dict}")

    # ë ˆì´ë¸” ì›-í•« ì¸ì½”ë”©
    num_classes = len(np.unique(y))
    y_encoded = to_categorical(y, num_classes=num_classes)
    
    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {X_train.shape}, ê²€ì¦ ë°ì´í„°: {X_test.shape}")
    
    # ëª¨ë¸ ìƒì„±
    model = create_model(num_classes)
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¶œë ¥
    print("\nğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜:")
    model.summary()
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        # Early Stopping: 10 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
        EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        
        # Model Checkpoint: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        ModelCheckpoint(
            save_path,
            save_best_only=True,
            monitor='val_loss',
            verbose=0
        ),
        
        # Learning Rate Reduction: 5 ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ í•™ìŠµë¥  ê°ì†Œ
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        )
    ]
    
    print(f"\nğŸš€ {model_name} í•™ìŠµ ì‹œì‘...")
    print("="*70)
    
    # ëª¨ë¸ í•™ìŠµ
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1
    )
    
    print(f"\nâœ… {model_name} ì €ì¥ ì™„ë£Œ: {save_path}")
    
    # TFLite ëª¨ë¸ ì €ì¥
    save_tflite_model(model, save_path)
    
    # ìƒì„¸ í‰ê°€
    print("\n" + "="*70)
    print("ğŸ” ëª¨ë¸ í‰ê°€ ì¤‘...")
    print("="*70)
    
    # label_map ì¬êµ¬ì„± (yì—ì„œ ì—­ìœ¼ë¡œ ì¶”ì¶œ)
    label_map = {}
    for i in range(num_classes):
        label_map[f"Class_{i}"] = i
    
    evaluate_model(model, X_test, y_test, label_map)
    
    return history, model

def plot_training_history(history):
    """
    í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”
    """
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Accuracy
    axes[0].plot(history.history['categorical_accuracy'], label='Train Acc', linewidth=2)
    axes[0].plot(history.history['val_categorical_accuracy'], label='Val Acc', linewidth=2)
    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
    axes[0].set_ylabel('Accuracy', fontsize=12)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].legend(loc='lower right')
    axes[0].grid(True, alpha=0.3)
    
    # Loss
    axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)
    axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')
    axes[1].set_ylabel('Loss', fontsize=12)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].legend(loc='upper right')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ì €ì¥
    history_path = os.path.join(MODELS_DIR, 'training_history.png')
    plt.savefig(history_path, dpi=150)
    print(f"âœ… í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥ë¨: {history_path}")
    plt.show()

def main():
    os.makedirs(MODELS_DIR, exist_ok=True)
    
    print("="*70)
    print("ğŸ§  LSTM Gesture Recognition Model Training")
    print("="*70)
    
    print("\nğŸ“‚ Legacy ë°ì´í„° ë¡œë”© ì¤‘...")
    X_legacy, y_legacy, label_map_legacy = load_data(LEGACY_DATA_DIR, apply_normalization=True)
    print(f"âœ… Legacy ë°ì´í„°: {X_legacy.shape}, í´ë˜ìŠ¤: {label_map_legacy}")
    
    # Train Legacy
    if len(X_legacy) > 0:
        save_path = os.path.join(MODELS_DIR, 'lstm_legacy.h5')
        history, model = train_model(
            X_legacy, y_legacy, save_path, "Legacy Model",
            apply_augmentation=True
        )
        
        # Plot training history
        if history:
            print("\nğŸ“ˆ í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™” ì¤‘...")
            plot_training_history(history)
            
        print("\n" + "="*70)
        print("ğŸ‰ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print("="*70)
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼:")
        print(f"   - {save_path}")
        print(f"   - {save_path.replace('.h5', '.tflite')}")
        print(f"   - {os.path.join(MODELS_DIR, 'confusion_matrix.png')}")
        print(f"   - {os.path.join(MODELS_DIR, 'training_history.png')}")
        
    else:
        print("âŒ No data found. Please collect data first using collect_mp_legacy.py")

if __name__ == "__main__":
    main()
